{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Python310\\lib\\site-packages\\transformers\\trainer_pt_utils.py:211: UserWarning: Failed to initialize NumPy: module compiled against API version 0x10 but this version of numpy is 0xf . Check the section C-API incompatibility at the Troubleshooting ImportError section at https://numpy.org/devdocs/user/troubleshooting-importerror.html#c-api-incompatibility for indications on how to solve this problem . (Triggered internally at ..\\torch\\csrc\\utils\\tensor_numpy.cpp:84.)\n",
      "  device: Optional[torch.device] = torch.device(\"cuda\"),\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, GPT2Config\n",
    "from transformers import TextDataset, DataCollatorForLanguageModeling, Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraping data with Selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import json\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_response(user_input, fantasy_data):\n",
    "#     # Tokenize user input\n",
    "#     input_ids = tokenizer.encode(user_input, return_tensors='pt')\n",
    "\n",
    "#     # Generate response using the model, incorporating fantasy data\n",
    "#     # Pass the fantasy data as input to the model along with the user input\n",
    "#     output = model.generate(input_ids, max_length=100, num_return_sequences=1,\n",
    "#                             no_repeat_ngram_size=2, fantasy_data=fantasy_data)\n",
    "\n",
    "#     # Decode and return the response\n",
    "#     response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "#     return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import GPT2LMHeadModel\n",
    "\n",
    "# class FantasyGPT2Model(GPT2LMHeadModel):\n",
    "#     def forward(self, input_ids, fantasy_data=None, **kwargs):\n",
    "#         # Embed the fantasy data and concatenate it with the input embeddings\n",
    "#         fantasy_embeddings = self.fantasy_embedding_layer(fantasy_data)\n",
    "#         input_embeddings = self.transformer.wte(input_ids)\n",
    "#         combined_embeddings = torch.cat([input_embeddings, fantasy_embeddings], dim=1)\n",
    "\n",
    "#         # Pass the combined embeddings through the transformer\n",
    "#         transformer_outputs = self.transformer(inputs_embeds=combined_embeddings, **kwargs)\n",
    "#         lm_logits = self.lm_head(transformer_outputs[0])\n",
    "\n",
    "#         return lm_logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "NFL_TEAMS = [\n",
    "    \"azcardinals\",\n",
    "    \"atlantafalcons\",\n",
    "    \"baltimoreravens\",\n",
    "    \"buffalobills\",\n",
    "    \"panthers\",\n",
    "    \"chicagobears\",\n",
    "    \"bengals\",\n",
    "    \"clevelandbrowns\",\n",
    "    \"dallascowboys\",\n",
    "    \"denverbroncos\",\n",
    "    \"detroitlions\",\n",
    "    \"packers\",\n",
    "    \"houstontexans\",\n",
    "    \"colts\",\n",
    "    \"jaguars\",\n",
    "    \"chiefs\",\n",
    "    \"raiders\",\n",
    "    \"chargers\",\n",
    "    \"therams\",\n",
    "    \"miamidolphins\",\n",
    "    \"vikings\",\n",
    "    \"patriots\",\n",
    "    \"neworleanssaints\",\n",
    "    \"giants\",\n",
    "    \"newyorkjets\",\n",
    "    \"philadelphiaeagles\",\n",
    "    \"steelers\",\n",
    "    \"49ers\",\n",
    "    \"seahawks\",\n",
    "    \"buccaneers\",\n",
    "    \"tennesseetitans\",\n",
    "    \"commanders\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_driver():\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")\n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "    return driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_nfl_team_stats(start_year):\n",
    "    '''\n",
    "    Collecting yearly NFL team stats, individual payer totals\n",
    "\n",
    "    Args:\n",
    "        driver (selenium)\n",
    "        start_year (int)\n",
    "    \n",
    "    Returns:\n",
    "        team_stats (dict)\n",
    "    '''\n",
    "    driver = new_driver()\n",
    "    team_stats = {}\n",
    "    for team in NFL_TEAMS:\n",
    "        for year in range(start_year, datetime.now().year):\n",
    "            try:\n",
    "                driver.get(f'https://www.{team}.com/team/stats/{year}/REG')\n",
    "            except Exception:\n",
    "                print(\"Error occurred. Creating a new driver instance...\")\n",
    "                driver.quit()\n",
    "                driver = new_driver()\n",
    "                driver.get(f'https://www.{team}.com/team/stats/{year}/REG')\n",
    "            team_stats[team + ' ' + str(year)] = collect_team(driver)\n",
    "\n",
    "    return team_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_team(soup):\n",
    "    # scraping stat names\n",
    "    html = soup.find_all('ul', class_='nfl-o-team-h2h-stats__list')\n",
    "    if len(html) > 0:\n",
    "        html = html[0]\n",
    "    else:\n",
    "        return {}\n",
    "    labels = []\n",
    "    rel_tags = ['nfl-o-team-h2h-stats__label--full',  'nfl-o-team-h2h-stats__label--first-child nfl-o-team-h2h-stats__label--child',\n",
    "    'nfl-o-team-h2h-stats__label--child', 'nfl-o-team-h2h-stats__label--last-child nfl-o-team-h2h-stats__label--child']\n",
    "    # dict describing how many children stats a descriptor has\n",
    "    descriptors = {'FIRST DOWNS': 3, 'OFFENSE': 2, 'RUSHING': 2, 'PASSING': 4, 'TDs': 4}\n",
    "    for tag in html.find_all('span'):\n",
    "        tag_classes = tag.get('class')\n",
    "        if not tag_classes or any(cls in rel_tags for cls in tag_classes):\n",
    "            text = tag.get_text(strip=True)\n",
    "            if not is_number(text):\n",
    "                labels.append(text)\n",
    "    # organizing stat names to be keys\n",
    "    labels[28] = 'TDs'\n",
    "    new_labels = []\n",
    "    descriptor_counter = 0\n",
    "    descriptor = ''\n",
    "    sub_list = []\n",
    "    for label in labels:\n",
    "        if label in ['Completions', 'Attempts', 'Interceptions', 'Average']:\n",
    "            continue\n",
    "        elif label in descriptors.keys() and descriptor_counter == 0:\n",
    "            descriptor_counter += descriptors[label]\n",
    "            descriptor = label\n",
    "        elif descriptor_counter == 0:\n",
    "            descriptor_list = []\n",
    "            new_labels.extend(with_opponent(label))\n",
    "        else:\n",
    "            new_labels.append(descriptor + ' ' + label)\n",
    "            sub_list.append(label)\n",
    "            descriptor_counter -= 1\n",
    "            if descriptor_counter == 0:\n",
    "                for sub in sub_list:\n",
    "                    new_labels.append(descriptor + ' ' + sub + ' Opponent')\n",
    "                sub_list = []\n",
    "\n",
    "    # getting the values of the stats   \n",
    "    # Find the value and label elements within each element\n",
    "    value_elements = html.find_all('div', class_='nfl-o-team-h2h-stats__value')\n",
    "    # Extract and print the values and labels\n",
    "    values = []\n",
    "    for value_element in value_elements:\n",
    "        if value_element.span:\n",
    "            # Extract each individual value from the <span> tags\n",
    "            span_values = [span.get_text(strip=True) for span in value_element.find_all('span')]\n",
    "            values.extend(span_values)\n",
    "        else:\n",
    "            # If there are no <span> tags, extract the text directly\n",
    "            value_text = value_element.get_text(strip=True)\n",
    "            values.append(value_text)\n",
    "\n",
    "    stats_dict = dict(zip(new_labels, values))\n",
    "    return stats_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def table_scraper(html):\n",
    "    data = {}\n",
    "    headers = None\n",
    "\n",
    "    for row in html.find_all('tr'):\n",
    "        cells = row.find_all(['th', 'td'])\n",
    "        if cells:\n",
    "            if headers is None:\n",
    "                headers = [cell.get_text(strip=True) for cell in cells[1:]]\n",
    "            else:\n",
    "                key = cells[0].get_text(strip=True)\n",
    "                values = [cell.get_text(strip=True) for cell in cells[1:]]\n",
    "                data[key] = dict(zip(headers, values))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_player(soup):\n",
    "    player_stats = soup.find_all('div', class_='nfl-o-teamstats')\n",
    "    passing = player_stats[0]\n",
    "    rushing = player_stats[1]\n",
    "    receiving = player_stats[2]\n",
    "    player_data = {}\n",
    "    player_data['passing'] = table_scraper(passing)\n",
    "    player_data['rushing'] = table_scraper(rushing)\n",
    "    player_data['receiving'] = table_scraper(receiving)\n",
    "    return player_data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_team(driver):\n",
    "    '''\n",
    "    parses html and produces json for the given year and team\n",
    "    '''\n",
    "    page_source = driver.page_source\n",
    "    soup = BeautifulSoup(page_source, 'html.parser')\n",
    "    team_stats = scrape_team(soup)\n",
    "    player_stats = scrape_player(soup)\n",
    "    team_stats.update(player_stats)\n",
    "    return team_stats\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_number(string):\n",
    "    try:\n",
    "        float(string)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def with_opponent(stat):\n",
    "    return [stat, stat + ' Opponent']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_fp(pos, year):\n",
    "    df = pd.read_html(f'https://www.fantasypros.com/nfl/stats/{pos}.php?year={year}scoring=HALF')[0]\n",
    "    # Create a new list of column names with concatenated levels\n",
    "    if pos in ['rb', 'wr', 'te', 'qb']:\n",
    "        new_columns = []\n",
    "        for col in df.columns:\n",
    "            new_col = ''\n",
    "            for level in col:\n",
    "                if 'Unnamed' in level:\n",
    "                    continue\n",
    "                else:\n",
    "                    new_col += level + ' '\n",
    "            \n",
    "            new_columns.append(new_col[:-1])\n",
    "\n",
    "        # Assign the new column names to the DataFrame\n",
    "        df.columns = new_columns\n",
    "    data = []\n",
    "    for _, row in df.iterrows():\n",
    "        player_data = {}\n",
    "        for column in df.columns:\n",
    "            player_data[column] = row[column]\n",
    "        data.append(player_data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_fantasy_pros(start_year=2002):\n",
    "    final_data = {}\n",
    "    for year in range(start_year, datetime.now().year):\n",
    "        for pos in ['rb', 'wr', 'te', 'k', 'dst']:\n",
    "            final_data[pos + ' ' + str(year)] = (scrape_fp(pos, year))\n",
    "    return final_data\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_consensus_rankings():\n",
    "    scoring_dict = {'consensus-cheatsheets': 'Standard Scoring',\n",
    "                    'half-point-ppr-cheatsheets': 'Half PPR',\n",
    "                    'ppr-cheatsheets': 'Full PPR'}\n",
    "    final_data = {}\n",
    "    for scoring_url in ['consensus-cheatsheets', 'half-point-ppr-cheatsheets', 'ppr-cheatsheets']:\n",
    "        driver = new_driver()\n",
    "        driver.get(f'https://www.fantasypros.com/nfl/rankings/{scoring_url}.php')\n",
    "        # Wait for the rankings table to load (assuming it's loaded dynamically)\n",
    "        table_locator = (By.CSS_SELECTOR, \"#ranking-table\")\n",
    "        driver.implicitly_wait(10)  # Wait for up to 10 seconds for the element to appear\n",
    "        table = driver.find_elements(*table_locator)\n",
    "        \n",
    "        # Scroll down to load all the records\n",
    "        table_element = table[0]\n",
    "        last_height = driver.execute_script(\"return arguments[0].scrollHeight\", table_element)\n",
    "        while True:\n",
    "            driver.execute_script(\"arguments[0].scrollTo(0, arguments[0].scrollHeight)\", table_element)\n",
    "            time.sleep(2)  # Adjust sleep time if needed\n",
    "            new_height = driver.execute_script(\"return arguments[0].scrollHeight\", table_element)\n",
    "            if new_height == last_height:\n",
    "                break\n",
    "            last_height = new_height\n",
    "\n",
    "        # Scrape the rankings\n",
    "        rankings = []\n",
    "        rows = table[0].find_elements(By.CSS_SELECTOR, \"tbody tr.player-row\")\n",
    "        for row in rows:\n",
    "            rank = row.find_element(By.CSS_SELECTOR, \"td.sticky-cell-one\").text\n",
    "            player = row.find_element(By.CSS_SELECTOR, \"td:nth-child(3) div.player-cell a\").text\n",
    "            team = row.find_element(By.CSS_SELECTOR, \"td:nth-child(3) div.player-cell span.player-cell-team\").text.strip(\"()\")\n",
    "            rankings.append({\"Rank\": rank, \"Player\": player, \"Team\": team})\n",
    "        final_data[scoring_dict[scoring_url]] = rankings\n",
    "    # Close the browser\n",
    "    driver.quit()\n",
    "    return final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "consensus_rankings = scrape_consensus_rankings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nfl_team_stats = collect_nfl_team_stats(2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "fantasy_pros = scrape_fantasy_pros(2002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consensus_rankings = scrape_consensus_rankings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_fp_text_from_dictionary(data):\n",
    "    output = \"\"\n",
    "    for position, players in data.items():\n",
    "        output += f\"{position.upper()}:\\n\"\n",
    "        for player in players:\n",
    "            output += f\"Rank: {player['Rank']}\\n\"\n",
    "            output += f\"Player: {player['Player']}\\n\"\n",
    "            for key, value in player.items():\n",
    "                if key not in ['Rank', 'Player']:\n",
    "                    output += f\"{key}: {value}\\n\"\n",
    "            output += \"\\n\"\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_team_text_from_dictionary(data):\n",
    "    output = \"\"\n",
    "    for team, stats in data.items():\n",
    "        output += f\"{team.upper()}:\\n\"\n",
    "        for key, value in stats.items():\n",
    "            if isinstance(value, dict):\n",
    "                output += f\"{key.capitalize()}:\\n\"\n",
    "                for player, player_stats in value.items():\n",
    "                    output += f\"{player}:\\n\"\n",
    "                    for stat, stat_value in player_stats.items():\n",
    "                        output += f\"{stat}: {stat_value}\\n\"\n",
    "                    output += \"\\n\"\n",
    "            else:\n",
    "                output += f\"{key}: {value}\\n\"\n",
    "        output += \"\\n\"\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_consensus(data):\n",
    "    output = \"\"\n",
    "    for scoring_type, players in data.items():\n",
    "        output += scoring_type + \":\\n\"\n",
    "        output += \"-\" * 30 + \"\\n\"\n",
    "        output += \"Rank  Player                 Team\\n\"\n",
    "        output += \"-\" * 30 + \"\\n\"\n",
    "        for player in players:\n",
    "            output += \"{:<6} {:<22} {}\\n\".format(player['Rank'], player['Player'], player['Team'])\n",
    "        output += \"\\n\"\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp_text = generate_fp_text_from_dictionary(fantasy_pros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "team_text = generate_team_text_from_dictionary(nfl_team_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "consensus_text = process_consensus(consensus_rankings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the string to a text file\n",
    "# filename = \"fp_text.txt\"\n",
    "# with open(filename, \"w\") as file:\n",
    "#     file.write(fp_text)\n",
    "\n",
    "# # Save the string to a text file\n",
    "# filename = \"team_text.txt\"\n",
    "# with open(filename, \"w\") as file:\n",
    "#     file.write(team_text)\n",
    "\n",
    "# # Save the string to a text file\n",
    "# filename = \"consensus_text.txt\"\n",
    "# with open(filename, \"w\") as file:\n",
    "#     file.write(consensus_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the string to a text file\n",
    "# filename = \"fantasy_pros.json\"\n",
    "# data_string = json.dumps(fantasy_pros)\n",
    "# with open(filename, \"w\") as file:\n",
    "#     file.write(data_string)\n",
    "\n",
    "# # Save the string to a text file\n",
    "# data_string = json.dumps(nfl_team_stats)\n",
    "# filename = \"nfl_team_stats.json\"\n",
    "# with open(filename, \"w\") as file:\n",
    "#     file.write(data_string)\n",
    "\n",
    "# # Save the string to a text file\n",
    "# data_string = json.dumps(consensus_rankings)\n",
    "# filename = \"consensus_rankings.json\"\n",
    "# with open(filename, \"w\") as file:\n",
    "#     file.write(data_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Load the pretrained model and tokenizer\n",
    "model_name = 'gpt2'\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "TextDataset.__init__() got an unexpected keyword argument 'text_string'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[76], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Create a text dataset using the fantasy football data\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m fp \u001b[39m=\u001b[39m TextDataset(\n\u001b[0;32m      3\u001b[0m     tokenizer\u001b[39m=\u001b[39;49mtokenizer,\n\u001b[0;32m      4\u001b[0m     file_path\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,  \u001b[39m# Pass None as we have the data in-memory\u001b[39;49;00m\n\u001b[0;32m      5\u001b[0m     text_string\u001b[39m=\u001b[39;49mfp_text,\n\u001b[0;32m      6\u001b[0m     block_size\u001b[39m=\u001b[39;49m\u001b[39m128\u001b[39;49m  \u001b[39m# Adjust the block size according to your data size\u001b[39;49;00m\n\u001b[0;32m      7\u001b[0m )\n\u001b[0;32m      9\u001b[0m \u001b[39m# Create a text dataset using the fantasy football data\u001b[39;00m\n\u001b[0;32m     10\u001b[0m consensus \u001b[39m=\u001b[39m TextDataset(\n\u001b[0;32m     11\u001b[0m     tokenizer\u001b[39m=\u001b[39mtokenizer,\n\u001b[0;32m     12\u001b[0m     file_path\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,  \u001b[39m# Pass None as we have the data in-memory\u001b[39;00m\n\u001b[0;32m     13\u001b[0m     text_string\u001b[39m=\u001b[39mconsensus_text,\n\u001b[0;32m     14\u001b[0m     block_size\u001b[39m=\u001b[39m\u001b[39m128\u001b[39m  \u001b[39m# Adjust the block size according to your data size\u001b[39;00m\n\u001b[0;32m     15\u001b[0m )\n",
      "\u001b[1;31mTypeError\u001b[0m: TextDataset.__init__() got an unexpected keyword argument 'text_string'"
     ]
    }
   ],
   "source": [
    "# Create a text dataset using the fantasy football data\n",
    "fp = TextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=None,  # Pass None as we have the data in-memory\n",
    "    text_string=fp_text,\n",
    "    block_size=128  # Adjust the block size according to your data size\n",
    ")\n",
    "\n",
    "# Create a text dataset using the fantasy football data\n",
    "consensus = TextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=None,  # Pass None as we have the data in-memory\n",
    "    text_string=consensus_text,\n",
    "    block_size=128  # Adjust the block size according to your data size\n",
    ")\n",
    "\n",
    "# Create a text dataset using the fantasy football data\n",
    "team = TextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=None,  # Pass None as we have the data in-memory\n",
    "    text_string=team_text,\n",
    "    block_size=128  # Adjust the block size according to your data size\n",
    ")\n",
    "\n",
    "# combined dataset\n",
    "combined_dataset = fp + consensus + team\n",
    "\n",
    "# Define the data collator for language modeling\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False  # Set to True if you want to apply masked language modeling\n",
    ")\n",
    "\n",
    "# Configure the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./retrained_model',  # Specify the output directory\n",
    "    overwrite_output_dir=True,  # Overwrite the content of the output directory\n",
    "    num_train_epochs=5, \n",
    "    per_device_train_batch_size=4, \n",
    "    save_steps=500,  # Save the model checkpoints at every specified number of steps\n",
    "    save_total_limit=2,  # Save a maximum of 2 checkpoints\n",
    ")\n",
    "\n",
    "# Initialize a new GPT-2 configuration\n",
    "config = GPT2Config.from_pretrained(model_name)\n",
    "\n",
    "# Instantiate a new GPT-2 model with the same configuration as the original model\n",
    "model = GPT2LMHeadModel(config=config)\n",
    "\n",
    "# Create a Trainer instance for training\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=combined_dataset\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the retrained model\n",
    "trainer.save_model('./retrained_model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'datasets'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[83], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdatasets\u001b[39;00m \u001b[39mimport\u001b[39;00m Dataset\n\u001b[0;32m      2\u001b[0m Dataset\u001b[39m.\u001b[39mfrom_dict(nfl_team_stats)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'datasets'"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "Dataset.from_dict(nfl_team_stats)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
